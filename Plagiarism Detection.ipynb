{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from random import randrange, shuffle\n",
    "from shutil import copyfile\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two sets of documents are used in this project: \n",
    "#### 1. Sample documents: documents that contain possibly plagiarised material\n",
    "#### 2. Original documents: excerpts from original Wikipedia articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP - 1: Shingling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing all sample documents' contents in a list of [(doc name, doc_contents)]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(<path to local directory that contains all the sample documents>)\n",
    "data_sample = []\n",
    "for file in os.listdir():\n",
    "    with open(file, encoding=\"utf8\", errors='ignore') as f:\n",
    "        data_sample.append((file,f.read().lower()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing all original documents' contents in a list of [(doc name, doc_contents)]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(<path to local directory that contains all the original documents>)\n",
    "original_sample = []\n",
    "for file in os.listdir():\n",
    "    with open(file, encoding=\"utf8\", errors='ignore') as f:\n",
    "        original_sample.append((file,f.read().lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic text cleaning - converting to lower case and removing all punctuations and special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    for ch in ['\\n','*','_','{','}','[',']','(',')','<','>','#','+','-','.','!','$','\\\\',',','?','\"','|','/','=']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch,\" \").strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_sample = [(x[0],clean_string(x[1])) for x in data_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_original_sample = [(x[0],clean_string(x[1])) for x in original_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for constructing n-shingles from the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(size,doc):\n",
    "    shingle_dict = {}\n",
    "    for i in range (0,len(doc)):\n",
    "        doc_shing = []\n",
    "        lenth = len(doc[i][1])\n",
    "        docid = doc[i][0]\n",
    "        # Tokenizing text to aid in the formation of shingles\n",
    "        tokens = doc[i][1].split()\n",
    "        for j in range (0,lenth-size+1):\n",
    "            sh = tuple(tokens[j:j+size])\n",
    "            if sh not in doc_shing:\n",
    "                doc_shing.append(sh)\n",
    "        # Final shingle dictionary: {\"docid\":[shingles]}        \n",
    "        shingle_dict[docid] = doc_shing\n",
    "    return shingle_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing 3-, 4- and 5-shingles and consolidating a list of these shingles from both the sample documents and the original documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7852"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing_data_3 = set(list(itertools.chain(*list(get_shingles(3, clean_data_sample).values()))))\n",
    "shing_orig_3 = set(list(itertools.chain(*list(get_shingles(3, clean_original_sample).values()))))\n",
    "shing_3 = shing_data_3 | shing_orig_3\n",
    "length_3 = len(shing_3)\n",
    "# Total number of 3-shingles in both sample and original data files\n",
    "length_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8786"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing_data_4 = set(list(itertools.chain(*list(get_shingles(4, clean_data_sample).values()))))\n",
    "shing_orig_4 = set(list(itertools.chain(*list(get_shingles(4, clean_original_sample).values()))))\n",
    "shing_4 = shing_data_4 | shing_orig_4\n",
    "length_4 = len(shing_4)\n",
    "# Total number of 4-shingles in both sample and original data files\n",
    "length_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9266"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing_data_5 = set(list(itertools.chain(*list(get_shingles(5, clean_data_sample).values()))))\n",
    "shing_orig_5 = set(list(itertools.chain(*list(get_shingles(5, clean_original_sample).values()))))\n",
    "shing_5 = shing_data_5 | shing_orig_5\n",
    "length_5 = len(shing_5)\n",
    "# Total number of 5-shingles in both sample and original data files\n",
    "length_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All sample documents and their constituent 5-shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bayes', 'theorem', 'is', 'an', 'important'),\n",
       " ('theorem', 'is', 'an', 'important', 'theorem'),\n",
       " ('is', 'an', 'important', 'theorem', 'relating'),\n",
       " ('an', 'important', 'theorem', 'relating', 'conditional'),\n",
       " ('important', 'theorem', 'relating', 'conditional', 'probabilities')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing_5_docs = get_shingles(5, clean_data_sample)\n",
    "shing_5_docs = collections.OrderedDict(list(shing_5_docs.items()))\n",
    "# Ordered dictionary for referencing documents and their shingles by index\n",
    "list(shing_5_docs.items())[0][1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique k-shingles for k={3,4,5}:\n",
    "1. k=3: 7852\n",
    "2. k=4: 8786\n",
    "3. k=5: 9266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('it', 'inherited', 'from', 'the', 'peropos'), 0),\n",
       " (('a', 'it', 'is', 'previous', 'in'), 1),\n",
       " (('person', 'may', 'be', 'seen', 'to'), 2),\n",
       " (('be', 'written', '7', 'so', 'this'), 3),\n",
       " (('the', 'philosophy', 'of', 'science', 'it'), 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing_5_list = list(shing_5)\n",
    "shing_5_ind = {k: v for v, k in enumerate(shing_5_list)}\n",
    "# Creating a dictionary of indexes for all 5-shingles\n",
    "list(shing_5_ind.items())[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP - 2: Min-Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate hashing functions using the (ax + b) mod N formula, \n",
    "# Parameters:\n",
    "# 1. N: the total number of 5-shingles in the sample and original documents combined.\n",
    "# 2. L: number of hash functions\n",
    "# Returns the generated hash functions: L new lists of size N\n",
    "def get_hash_functions(N,L):\n",
    "    hash_functions = []\n",
    "    \n",
    "    for itr in range(L):\n",
    "        # a and b are random numbers\n",
    "        a=randrange(1,400)\n",
    "        b=randrange(1,400)\n",
    "        \n",
    "        new_hash_function = []\n",
    "        for i in range(N):\n",
    "            # Implementing the (ax + b) mod N formula:\n",
    "            new_hash_function.append((a * i + b) % N)\n",
    "        \n",
    "        hash_functions.append(new_hash_function)\n",
    "    return hash_functions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating hash functions for only shingle index created for k=5\n",
    "#L = {50,100,200,500,1000}\n",
    "N = length_5\n",
    "hash_func50 = get_hash_functions(N, 50)\n",
    "hash_func100 = get_hash_functions(N, 100)\n",
    "hash_func200 = get_hash_functions(N, 200)\n",
    "hash_func500 = get_hash_functions(N, 500)\n",
    "hash_func1000 = get_hash_functions(N, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for composing the signature matrix. Spits out the signature matrix in transposed form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates minhash values for each document, over all hash documents, avoiding the need for an input matrix\n",
    "# Parameters:\n",
    "# 1. List of hash functions\n",
    "# 2. Number of hash functions\n",
    "# 3. Dictionary of 5-shingles from sample documents\n",
    "# 4. Dictionary of indexes of all shingles\n",
    "# Returns signature matrix of dimensions: rows(# of documents) x columns(# of hash functions used, minhash values)\n",
    "def get_sig_m(hash_f, L, sh_5, sh_5_index):\n",
    "    sig_m = []\n",
    "    i = 0\n",
    "    # Iterating through sample documents:\n",
    "    for k in sh_5.values():\n",
    "        sig_list = list(np.ones(L) * np.inf)\n",
    "        # Iterating through all shingles:\n",
    "        for sh, v in sh_5_index.items():\n",
    "            # Where a shingle from the dictionary of indexes is found in the document, determine\n",
    "            # the minshash value\n",
    "            if sh in k:\n",
    "                for j in range(L):\n",
    "                    if hash_f[j][v] < sig_list[j]:\n",
    "                        sig_list[j] = hash_f[j][v]\n",
    "        i = i + 1\n",
    "        sig_m.append(sig_list)\n",
    "    return np.array(sig_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_50 = get_sig_m(hash_func50,50, shing_5_docs,shing_5_ind)\n",
    "signature_100 = get_sig_m(hash_func100,100, shing_5_docs,shing_5_ind)\n",
    "signature_200 = get_sig_m(hash_func200,200, shing_5_docs,shing_5_ind)\n",
    "signature_500 = get_sig_m(hash_func500,500, shing_5_docs,shing_5_ind)\n",
    "signature_1000 = get_sig_m(hash_func1000,1000,shing_5_docs,shing_5_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact-checking with one file from the original sample directory\n",
    "original = 'Original_Sample/orig_taskc.txt'\n",
    "# Jaccard similarity threshold\n",
    "t = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: All original documents and their constituent 5-shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 'probability', 'theory', \"bayes'\", 'theorem'),\n",
       " ('probability', 'theory', \"bayes'\", 'theorem', 'often'),\n",
       " ('theory', \"bayes'\", 'theorem', 'often', 'called'),\n",
       " (\"bayes'\", 'theorem', 'often', 'called', \"bayes'\"),\n",
       " ('theorem', 'often', 'called', \"bayes'\", 'law')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_5_orig = get_shingles(5, clean_original_sample)\n",
    "sh_5_orig = collections.OrderedDict(sh_5_orig)\n",
    "# Creating a dictionary of indexes for all 5-shingles\n",
    "list(sh_5_orig.items())[0][1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the jaccard distance: intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for calculating the similarity of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gauges similarity of documents by calculating the jaccard similarity of \n",
    "# sets of minhash values of documents\n",
    "# Parameters:\n",
    "# 1. (Transposed) signature matrix of sample documents\n",
    "# 2. (Transposed) signature matrix of original documents\n",
    "# 3. Threshold\n",
    "# Returns a list of sample-original document pairs with a jaccard similarity greater than the supplied threshold\n",
    "\n",
    "# List of the shingles from the sample documents and the original documents\n",
    "da = list(shing_5_docs.items())\n",
    "ori = list(sh_5_orig.items())\n",
    "def sim_docs(sign_samp, sign_orig, t):\n",
    "    sim = collections.defaultdict()\n",
    "    # Calculating the jaccard similarities of all sample documents against all original documents\n",
    "    for i in range(0,len(sign_samp)):\n",
    "        for j in range(0,len(sign_orig)):\n",
    "            card = jaccard(sign_samp[i].tolist(),sign_orig[j].tolist())\n",
    "            # If the similarity score is greater than the threshold, output\n",
    "            if card >= t:\n",
    "                d = da[i][0]\n",
    "                o = ori[j][0]\n",
    "                k = d+\" \"+o\n",
    "                sim[d+\" & \"+o] = card\n",
    "    return sorted(sim.items(), key=lambda k_v: k_v[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Computing signature matrices for original documents with a varied number of hash functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_50_orig = get_sig_m(hash_func50,50,sh_5_orig,shing_5_ind)\n",
    "signature_100_orig = get_sig_m(hash_func100,100,sh_5_orig,shing_5_ind)\n",
    "signature_200_orig = get_sig_m(hash_func200,200,sh_5_orig,shing_5_ind)\n",
    "signature_500_orig = get_sig_m(hash_func500,500,sh_5_orig,shing_5_ind)\n",
    "signature_1000_orig = get_sig_m(hash_func1000,1000,sh_5_orig,shing_5_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each L = {50,100,200,500,1000}, all documents listed under have Jaccard similarity > t=0.2, and are sorted in decreasing order of Jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Documents that have a similarity greater than the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g0pE_taska.txt & orig_taska.txt', 0.5625),\n",
       " ('g4pC_taska.txt & orig_taska.txt', 0.5384615384615384),\n",
       " ('g3pA_taskd.txt & orig_taskd.txt', 0.47058823529411764),\n",
       " ('g4pC_taskd.txt & orig_taskd.txt', 0.4084507042253521),\n",
       " ('g0pD_taska.txt & orig_taska.txt', 0.2987012987012987)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_docs(signature_50, signature_50_orig, 0.2)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g0pE_taska.txt & orig_taska.txt', 0.3888888888888889),\n",
       " ('g4pC_taska.txt & orig_taska.txt', 0.36054421768707484),\n",
       " ('g3pA_taskd.txt & orig_taskd.txt', 0.2987012987012987),\n",
       " ('g0pC_taska.txt & orig_taska.txt', 0.27388535031847133),\n",
       " ('g2pC_taska.txt & orig_taska.txt', 0.26582278481012656)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_docs(signature_100, signature_100_orig, 0.2)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g3pA_taskd.txt & orig_taskd.txt', 0.2345679012345679),\n",
       " ('g0pE_taska.txt & orig_taska.txt', 0.21212121212121213),\n",
       " ('g4pC_taska.txt & orig_taska.txt', 0.20481927710843373),\n",
       " ('g1pB_taska.txt & orig_taskd.txt', 0.2012012012012012)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_docs(signature_200, signature_200_orig, 0.2)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g0pE_taska.txt & orig_taska.txt', 0.11234705228031146),\n",
       " ('g4pC_taska.txt & orig_taska.txt', 0.10741971207087486),\n",
       " ('g0pE_taske.txt & orig_taska.txt', 0.10619469026548672),\n",
       " ('g1pD_taskd.txt & orig_taska.txt', 0.10619469026548672),\n",
       " ('g0pE_taskd.txt & orig_taska.txt', 0.10497237569060773)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_docs(signature_500, signature_500_orig, 0.1)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g0pE_taskd.txt & orig_taska.txt', 0.06553010122535961),\n",
       " ('g3pC_taskd.txt & orig_taska.txt', 0.06496272630457935),\n",
       " ('g0pE_taske.txt & orig_taska.txt', 0.06439595529536987),\n",
       " ('g3pB_taske.txt & orig_taska.txt', 0.06439595529536987),\n",
       " ('g0pD_taske.txt & orig_taska.txt', 0.06439595529536987)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_docs(signature_1000, signature_1000_orig, 0.05)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP - 3: LSH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, to hash signature matrix into B buckets\n",
    "# Using the technique of splitting the signature matrix into b bands of r rows\n",
    "# Converting only the signature matrix generated with L=1000\n",
    "b = 50\n",
    "r = 20\n",
    "B = 199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segregating the signature function into b bands of r rows each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes the signature matrix in transposed form, and splits every document's \n",
    "# minhash values into b bands of r rows\n",
    "# Parameters:\n",
    "# 1. Signature matrix in transposed form\n",
    "# 2. Number of bands\n",
    "# 3. Number of rows in each band\n",
    "# Returns a matrix of separated bands\n",
    "def bands(sig,b,r):\n",
    "    l = len(sig[0])\n",
    "    x_banded = []\n",
    "    for doc in sig:\n",
    "        x_bands = []\n",
    "        n = 0\n",
    "        while n <= l-r:\n",
    "            x_bands.append(tuple(doc[n:n+r]))\n",
    "            n = n+r\n",
    "        x_banded.append(x_bands)\n",
    "    return np.array(x_banded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to hash the values in the b bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function hashes the minhashes values using the formula: (sum over band(minhash value*random integer a)) % N\n",
    "# Parameters:\n",
    "# 1. Banded matrix from previous step\n",
    "# 2. Number of buckets, N\n",
    "def hash_vals(band_mat, N):\n",
    "    # Generating a list of random integers between 1 and 20 that will be used across bands\n",
    "    a = list(range(1,r+1))\n",
    "    shuffle(a)\n",
    "    lsh = []\n",
    "    for doc in band_mat:\n",
    "        hashed = []\n",
    "        for v in doc:\n",
    "            # Hashing the minhash values into buckets\n",
    "            summed = sum([v[x]*a[x]for x in range(r)])%N\n",
    "            hashed.append(summed)\n",
    "        lsh.append(hashed)\n",
    "    return np.array(lsh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Banding and hashing the sample documents' and original documents' minhash signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "banded_mat = bands(signature_1000,b,r)\n",
    "hashed_docs = hash_vals(banded_mat,B)\n",
    "banded_mat_orig = bands(signature_1000_orig,b,r)\n",
    "hashed_docs_orig = hash_vals(banded_mat_orig,B)\n",
    "# One document from the set of original documents available\n",
    "test_orig = hashed_docs_orig[1].tolist()\n",
    "# Name of the aforementioned original document\n",
    "orig_name = list(sh_5_orig.items())[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Function for Locality Sensitive Hashing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the banded and hashed matrices for the sample and original documents and \n",
    "# computes similarity based on the principal that similar documents are likely to end up in\n",
    "# the same bucket\n",
    "# Parameters:\n",
    "# 1. Banded + hashed sample documents\n",
    "# 2. Dictionary of sample documents and constiuent 5-shingles\n",
    "# 3. Original banded + hashed document\n",
    "# 4. Threshold\n",
    "# Returns:\n",
    "# 1. Candidates (at least one same hash value shared between sample document and original document)\n",
    "# 2. False negatives\n",
    "# 3. False positives\n",
    "# 4. Jaccard similarity scores of candidate documents\n",
    "def local_sh(hash_docs, sh_5, test_o,t):\n",
    "    cand = []\n",
    "    jaccard_s = []\n",
    "    false_neg = []\n",
    "    false_pos = []\n",
    "    ind = 0\n",
    "    for x in hash_docs:\n",
    "        fl = 0\n",
    "        # Name of the sample document\n",
    "        doc = list(sh_5.items())[ind][0]\n",
    "        # A document is considered a candidate only when at least one of its banded+hashed value\n",
    "        # is the shared with the banded+hashed values of the original document\n",
    "        if len([i for i, j in zip(x, test_o) if i == j]) >= 1:\n",
    "            cand.append(doc)\n",
    "            fl = 1\n",
    "        j = jaccard(list(x),test_o)\n",
    "        if fl > 0:\n",
    "            # Jaccard similarity scores of all candidate documents\n",
    "            jaccard_s.append((doc,orig_name,j))\n",
    "        # False negatives are those documents whose similarity scores are greater than the\n",
    "        # threshold, but are not indentified as candidates\n",
    "        if j > t and doc not in cand:\n",
    "            false_neg.append(doc)\n",
    "        # False positives are those documents whose similarity scores are lower than the\n",
    "        # threshold, but are indentified as candidates\n",
    "        elif j <= t and doc in cand:\n",
    "            false_pos.append(doc)\n",
    "        ind = ind+1\n",
    "    jaccard_s.sort(key=lambda x:x[2], reverse=True)\n",
    "    return (cand, false_neg, false_pos, jaccard_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates, false_negatives, false_positives, jaccards = local_sh(hashed_docs,shing_5_docs,test_orig,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g0pE_taskd.txt',\n",
       " 'g1pA_taske.txt',\n",
       " 'g1pA_taska.txt',\n",
       " 'g0pB_taska.txt',\n",
       " 'g0pB_taske.txt',\n",
       " 'g4pE_taske.txt',\n",
       " 'g0pA_taska.txt',\n",
       " 'g2pB_taska.txt',\n",
       " 'g4pE_taska.txt',\n",
       " 'g2pE_taskd.txt',\n",
       " 'g1pD_taska.txt',\n",
       " 'g2pE_taske.txt',\n",
       " 'g3pA_taskd.txt',\n",
       " 'g4pB_taskd.txt',\n",
       " 'g4pB_taske.txt',\n",
       " 'g1pD_taske.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g4pE_taske.txt', 'orig_taske.txt', 0.20481927710843373),\n",
       " ('g2pE_taske.txt', 'orig_taske.txt', 0.17647058823529413),\n",
       " ('g1pA_taske.txt', 'orig_taske.txt', 0.16279069767441862),\n",
       " ('g0pB_taska.txt', 'orig_taske.txt', 0.16279069767441862),\n",
       " ('g0pB_taske.txt', 'orig_taske.txt', 0.16279069767441862),\n",
       " ('g1pD_taska.txt', 'orig_taske.txt', 0.13636363636363635),\n",
       " ('g0pE_taskd.txt', 'orig_taske.txt', 0.12359550561797752),\n",
       " ('g1pA_taska.txt', 'orig_taske.txt', 0.12359550561797752),\n",
       " ('g2pE_taskd.txt', 'orig_taske.txt', 0.12359550561797752),\n",
       " ('g4pB_taskd.txt', 'orig_taske.txt', 0.12359550561797752),\n",
       " ('g4pE_taska.txt', 'orig_taske.txt', 0.1111111111111111),\n",
       " ('g4pB_taske.txt', 'orig_taske.txt', 0.0989010989010989),\n",
       " ('g2pB_taska.txt', 'orig_taske.txt', 0.08695652173913043),\n",
       " ('g3pA_taskd.txt', 'orig_taske.txt', 0.08695652173913043),\n",
       " ('g0pA_taska.txt', 'orig_taske.txt', 0.07526881720430108),\n",
       " ('g1pD_taske.txt', 'orig_taske.txt', 0.06382978723404255)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g0pE_taskd.txt',\n",
       " 'g1pA_taske.txt',\n",
       " 'g1pA_taska.txt',\n",
       " 'g0pB_taska.txt',\n",
       " 'g0pB_taske.txt',\n",
       " 'g0pA_taska.txt',\n",
       " 'g2pB_taska.txt',\n",
       " 'g4pE_taska.txt',\n",
       " 'g2pE_taskd.txt',\n",
       " 'g1pD_taska.txt',\n",
       " 'g2pE_taske.txt',\n",
       " 'g3pA_taskd.txt',\n",
       " 'g4pB_taskd.txt',\n",
       " 'g4pB_taske.txt',\n",
       " 'g1pD_taske.txt']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with different values of b, r, B to reduce the number of false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('g3pB_taska.txt', 'orig_taske.txt', 0.1111111111111111), ('g0pE_taskd.txt', 'orig_taske.txt', 0.05263157894736842), ('g0pC_taska.txt', 'orig_taske.txt', 0.05263157894736842), ('g3pC_taskd.txt', 'orig_taske.txt', 0.05263157894736842)]\n",
      "Candidates: 4 , False negatives: 0 , False positives: 4\n"
     ]
    }
   ],
   "source": [
    "b = 10\n",
    "r = 100\n",
    "B = 200\n",
    "t = 0.2\n",
    "banded_mat = bands(signature_1000,b,r)\n",
    "hashed_docs = hash_vals(banded_mat,B)\n",
    "banded_mat_orig = bands(signature_1000_orig,b,r)\n",
    "hashed_docs_orig = hash_vals(banded_mat_orig,B)\n",
    "test_orig = hashed_docs_orig[1].tolist()\n",
    "orig_name = list(sh_5_orig.items())[1][0]\n",
    "\n",
    "candidates, false_negatives, false_positives, jaccards = local_sh(hashed_docs,shing_5_docs,test_orig,t)\n",
    "len_candidates = len(candidates)\n",
    "len_fn = len(false_negatives)\n",
    "len_fp = len(false_positives)\n",
    "print(jaccards)\n",
    "print(\"Candidates:\",len_candidates,\", False negatives:\",len_fn,\", False positives:\",len_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('g1pB_taske.txt', 'orig_taske.txt', 0.19760479041916168), ('g0pC_taska.txt', 'orig_taske.txt', 0.19047619047619047), ('g0pC_taskd.txt', 'orig_taske.txt', 0.17647058823529413), ('g2pB_taskd.txt', 'orig_taske.txt', 0.17647058823529413), ('g2pC_taske.txt', 'orig_taske.txt', 0.17647058823529413), ('g1pD_taskd.txt', 'orig_taske.txt', 0.17647058823529413), ('g0pE_taske.txt', 'orig_taske.txt', 0.16279069767441862), ('g3pB_taske.txt', 'orig_taske.txt', 0.16279069767441862), ('g0pD_taska.txt', 'orig_taske.txt', 0.16279069767441862), ('g1pA_taskd.txt', 'orig_taske.txt', 0.15606936416184972), ('g4pC_taske.txt', 'orig_taske.txt', 0.15606936416184972), ('g2pE_taske.txt', 'orig_taske.txt', 0.15606936416184972), ('g2pB_taska.txt', 'orig_taske.txt', 0.14942528735632185), ('g0pB_taske.txt', 'orig_taske.txt', 0.14285714285714285), ('g0pD_taskd.txt', 'orig_taske.txt', 0.12359550561797752)]\n",
      "Candidates: 15 , False negatives: 1 , False positives: 15\n"
     ]
    }
   ],
   "source": [
    "b = 100\n",
    "r = 10\n",
    "B = 250\n",
    "t = 0.2\n",
    "banded_mat = bands(signature_1000,b,r)\n",
    "hashed_docs = hash_vals(banded_mat,B)\n",
    "banded_mat_orig = bands(signature_1000_orig,b,r)\n",
    "hashed_docs_orig = hash_vals(banded_mat_orig,B)\n",
    "test_orig = hashed_docs_orig[1].tolist()\n",
    "orig_name = list(sh_5_orig.items())[1][0]\n",
    "\n",
    "candidates, false_negatives, false_positives, jaccards = local_sh(hashed_docs,shing_5_docs,test_orig,t)\n",
    "len_candidates = len(candidates)\n",
    "len_fn = len(false_negatives)\n",
    "len_fp = len(false_positives)\n",
    "print(jaccards)\n",
    "print(\"Candidates:\",len_candidates,\", False negatives:\",len_fn,\", False positives:\",len_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('g4pC_taskd.txt', 'orig_taske.txt', 0.3333333333333333)]\n",
      "Candidates: 1 , False negatives: 0 , False positives: 0\n"
     ]
    }
   ],
   "source": [
    "b = 2\n",
    "r = 500\n",
    "B = 500\n",
    "t = 0.3\n",
    "banded_mat = bands(signature_1000,b,r)\n",
    "hashed_docs = hash_vals(banded_mat,B)\n",
    "banded_mat_orig = bands(signature_1000_orig,b,r)\n",
    "hashed_docs_orig = hash_vals(banded_mat_orig,B)\n",
    "test_orig = hashed_docs_orig[1].tolist()\n",
    "orig_name = list(sh_5_orig.items())[1][0]\n",
    "\n",
    "candidates, false_negatives, false_positives, jaccards = local_sh(hashed_docs,shing_5_docs,test_orig,t)\n",
    "len_candidates = len(candidates)\n",
    "len_fn = len(false_negatives)\n",
    "len_fp = len(false_positives)\n",
    "print(jaccards)\n",
    "print(\"Candidates:\",len_candidates,\", False negatives:\",len_fn,\", False positives:\",len_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: 8 , False negatives: 7 , False positives: 6\n"
     ]
    }
   ],
   "source": [
    "b = 40\n",
    "r = 25\n",
    "B = 200\n",
    "t = 0.1\n",
    "banded_mat = bands(signature_1000,b,r)\n",
    "hashed_docs = hash_vals(banded_mat,B)\n",
    "banded_mat_orig = bands(signature_1000_orig,b,r)\n",
    "hashed_docs_orig = hash_vals(banded_mat_orig,B)\n",
    "test_orig = hashed_docs_orig[1].tolist()\n",
    "orig_name = list(sh_5_orig.items())[1][0]\n",
    "\n",
    "candidates, false_negatives, false_positives, jaccards = local_sh(hashed_docs,shing_5_docs,test_orig,t)\n",
    "len_candidates = len(candidates)\n",
    "len_fn = len(false_negatives)\n",
    "len_fp = len(false_positives)\n",
    "#print(jaccards)\n",
    "print(\"Candidates:\",len_candidates,\", False negatives:\",len_fn,\", False positives:\",len_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
